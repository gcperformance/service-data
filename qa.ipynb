{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444df633-bbf1-4bd6-98b3-4873f86a42ba",
   "metadata": {},
   "source": [
    "# Quality assurance and automated service data review\n",
    "\n",
    "This notebook reviews published service data for common mistakes. Relies on gc-service-data-script outputs to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b378e41d-fa09-48a8-b639-e18450cded57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pytz\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for JupyterLab progress bars\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3872ae-3955-4d82-a897-04bc4ccbf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datetime: 2024-12-02_11:45:08\n"
     ]
    }
   ],
   "source": [
    "# Specify date and time in correct timezone\n",
    "timezone = pytz.timezone('America/Montreal')\n",
    "current_datetime = pd.Timestamp.now(tz=timezone)\n",
    "current_datetime_str = current_datetime.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "print(f'Current datetime: {current_datetime_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bc87f5-686b-4e19-9bde-e154b2e4f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12477 entries, 0 to 12476\n",
      "Data columns (total 27 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   fiscal_yr                   12477 non-null  object \n",
      " 1   service_id                  12477 non-null  object \n",
      " 2   service_name_en             12477 non-null  object \n",
      " 3   service_name_fr             12477 non-null  object \n",
      " 4   service_standard_id         12477 non-null  object \n",
      " 5   service_standard_en         12459 non-null  object \n",
      " 6   service_standard_fr         12406 non-null  object \n",
      " 7   type                        12477 non-null  object \n",
      " 8   gcss_tool_fiscal_yr         5724 non-null   object \n",
      " 9   channel                     12477 non-null  object \n",
      " 10  channel_comments_en         4286 non-null   object \n",
      " 11  channel_comments_fr         4326 non-null   object \n",
      " 12  target_type                 10424 non-null  object \n",
      " 13  target                      12195 non-null  float64\n",
      " 14  volume_meeting_target       11425 non-null  float64\n",
      " 15  total_volume                11543 non-null  float64\n",
      " 16  performance                 9160 non-null   float64\n",
      " 17  comments_en                 4584 non-null   object \n",
      " 18  comments_fr                 4589 non-null   object \n",
      " 19  target_met                  9938 non-null   object \n",
      " 20  standards_targets_uri_en    8735 non-null   object \n",
      " 21  standards_targets_uri_fr    8761 non-null   object \n",
      " 22  performance_results_uri_en  2775 non-null   object \n",
      " 23  performance_results_uri_fr  2916 non-null   object \n",
      " 24  org_id                      12477 non-null  float64\n",
      " 25  department_en               12477 non-null  object \n",
      " 26  department_fr               12477 non-null  object \n",
      "dtypes: float64(5), object(22)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import service inventory and service standards to dataframes\n",
    "si = pd.read_csv('si.csv', sep=';')\n",
    "ss = pd.read_csv('ss.csv', sep=';')\n",
    "\n",
    "# Extract date of generation from timestamp on last line\n",
    "date = pd.to_datetime(si.iloc[-1, 0].split(':')[1].split('_')[0])\n",
    "\n",
    "# Remove last line with datestamp from dataframes\n",
    "si = si.iloc[:-1]\n",
    "ss = ss.iloc[:-1]\n",
    "\n",
    "ss.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4200ca-8e4e-417f-a153-6158a1f6fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca20a9b93984a1482b2189beda4e2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating URIs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All URI checks complete. 3763 were valid\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a URI's format is valid\n",
    "def is_valid_format(uri):\n",
    "    if not uri or not isinstance(uri, str):  # Ignore blank or non-string URIs\n",
    "        return False\n",
    "    return uri.startswith(('http://', 'https://'))  # Ensure valid web URL scheme\n",
    "\n",
    "# Function to check if a URI's address is valid\n",
    "def is_valid_uri(uri):\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        response = session.head(uri, allow_redirects=True, timeout=60)\n",
    "        return uri, response.status_code == 200\n",
    "    except requests.RequestException:  # Handle exceptions\n",
    "        return uri, False\n",
    "\n",
    "# Load existing validation results (if the file exists)\n",
    "try:\n",
    "    previous_results_df = pd.read_csv('uri_validation_results.csv')\n",
    "    previous_validation_results = dict(zip(previous_results_df['uri'], previous_results_df['is_valid']))\n",
    "except FileNotFoundError:\n",
    "    # If no previous results file exists, start with an empty dictionary\n",
    "    previous_validation_results = {}\n",
    "\n",
    "# Define URI columns and their corresponding DataFrames\n",
    "uri_cols = {\n",
    "    'service_uri_en': si,\n",
    "    'service_uri_fr': si,\n",
    "    'standards_targets_uri_en': ss,\n",
    "    'standards_targets_uri_fr': ss,\n",
    "    'performance_results_uri_en': ss,\n",
    "    'performance_results_uri_fr': ss\n",
    "}\n",
    "\n",
    "# Step 1: Consolidate all URIs into a single list\n",
    "all_uris = pd.concat([df[col] for col, df in uri_cols.items()], ignore_index=True)\n",
    "unique_uris = all_uris.dropna().unique()  # Drop NaN and get unique URIs\n",
    "valid_format_uris = [uri for uri in unique_uris if is_valid_format(uri)]  # Filter valid formats\n",
    "\n",
    "# Step 2: Identify new URIs that need validation\n",
    "new_uris = [uri for uri in valid_format_uris if uri not in previous_validation_results]\n",
    "\n",
    "print(f'checking {len(new_uris)}')\n",
    "\n",
    "# Step 3: Validate new URIs with multithreading\n",
    "validation_results = {}\n",
    "with ThreadPoolExecutor(max_workers=500) as executor:  # Adjust max_workers based on system capacity\n",
    "    future_to_uri = {executor.submit(is_valid_uri, uri): uri for uri in new_uris}\n",
    "    for future in tqdm(as_completed(future_to_uri), total=len(new_uris), desc=\"Validating URIs\"):\n",
    "        uri, is_valid = future.result()\n",
    "        validation_results[uri] = is_valid\n",
    "\n",
    "# Combine previous and new results\n",
    "validation_results.update(previous_validation_results)\n",
    "\n",
    "# Step 4: Map validation results back to each DataFrame column\n",
    "for column, df in uri_cols.items():\n",
    "    df[f'qa_{column}_is_valid'] = (\n",
    "        df[column]\n",
    "        .map(validation_results)\n",
    "        .astype(bool)\n",
    "        .fillna(False) # Replace NaN with False\n",
    "    )\n",
    "\n",
    "# Step 5: Save updated validation results to a CSV file\n",
    "validation_results_df = pd.DataFrame(list(validation_results.items()), columns=['uri', 'is_valid'])\n",
    "validation_results_df.to_csv('uri_validation_results.csv', index=False)\n",
    "\n",
    "valid_results = validation_results_df['is_valid'][validation_results_df['is_valid']].count()\n",
    "\n",
    "print(f\"All URI checks complete. {valid_results} were valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d98fc3e1-7124-43ee-a9e8-5628faed5e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('4', 'Office of the Commissioner for Federal Judicial Affairs Canada'), ('2', 'Office of the Commissioner for Federal Judicial Affairs Canada'), ('7', 'Office of the Commissioner for Federal Judicial Affairs Canada'), ('3', 'Office of the Commissioner for Federal Judicial Affairs Canada'), ('1', 'Office of the Commissioner for Federal Judicial Affairs Canada')}\n"
     ]
    }
   ],
   "source": [
    "# Duplicate service ID conflict\n",
    "# Step 1: Flag rows where 'service_id' is duplicated within each 'fiscal_yr'\n",
    "si['qa_duplicate_sid'] = si.duplicated(subset=['fiscal_yr', 'service_id'], keep=False)\n",
    "\n",
    "# Step 2: Get unique 'service_id's that are flagged as duplicates\n",
    "duplicate_ids = si.loc[si['qa_duplicate_sid'], 'service_id'].unique()\n",
    "\n",
    "# Step 3: Filter rows with duplicate 'service_id's and group by 'service_id' and 'department_en'\n",
    "duplicate_groups = (\n",
    "    si.loc[si['service_id'].isin(duplicate_ids), ['fiscal_yr', 'service_id', 'department_en']]\n",
    "    .groupby(['service_id', 'department_en'])['fiscal_yr']  # Count occurrences of 'fiscal_yr'\n",
    "    .nunique()  # Count unique fiscal years for each group\n",
    ")\n",
    "\n",
    "# Step 4: Identify groups with only one unique fiscal year (problematic cases)\n",
    "problematic_duplicates = duplicate_groups[duplicate_groups == 1].reset_index()\n",
    "\n",
    "# Step 5: Keep only 'service_id' and 'department_en' columns\n",
    "problematic_duplicates = problematic_duplicates[['service_id', 'department_en']]\n",
    "\n",
    "# Step 6: Create a set of tuples from 'problematic_duplicates' for efficient lookup\n",
    "problematic_set = set(zip(problematic_duplicates['service_id'], problematic_duplicates['department_en']))\n",
    "\n",
    "# Step 7: Update the 'qa_duplicate_sid' column based on whether each row matches a problematic duplicate\n",
    "si['qa_duplicate_sid'] = si.apply(\n",
    "    lambda row: (row['service_id'], row['department_en']) in problematic_set, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# check:\n",
    "#si.loc[:, ['fiscal_yr', 'department_en', 'service_id', 'qa_duplicate_sid']][si['qa_duplicate_sid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef63df1-843b-4590-86ba-63cacb931aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Service Standard ID conflict\n",
    "\n",
    "ss['qa_duplicate_stdid'] = ss.duplicated(subset=['fiscal_yr', 'service_standard_id'], keep=False)\n",
    "\n",
    "#ss.loc[:, ['fiscal_yr', 'department_en', 'service_id','service_standard_id', 'qa_duplicate_stdid']][ss['qa_duplicate_stdid']].sort_values(by='service_standard_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952cff2c-5b01-4b80-b4c3-47c4c266f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record is reported for a fiscal year that is incomplete or in the future.\n",
    "si['fiscal_yr_end_date'] = pd.to_datetime(si['fiscal_yr'].str.split('-').str[1]+'-04-01')\n",
    "si['qa_fiscal_yr_in_future'] = si['fiscal_yr_end_date'] >= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9ad843-c9aa-40ea-a1fd-2d8d8885f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record has contradiction between client feedback channels and online interaction points for feedback\n",
    "si['qa_client_feedback_contradiction'] = (\n",
    "\n",
    "    # Service accepts client feedback via the online channel (ONL) but online issue resolution or feedback is not applicable or not activated\n",
    "    (\n",
    "        si['client_feedback_channel'].str.contains('ONL') & \n",
    "        (\n",
    "            si['os_issue_resolution_feedback'].isna() | \n",
    "            (si['os_issue_resolution_feedback'] == 'N')\n",
    "        )\n",
    "    ) |\n",
    "    # Service has not listed the online channel (ONL) for client feedback but online issue resolution or feedback is activated\n",
    "    (\n",
    "        (~si['client_feedback_channel'].str.contains('ONL')) &\n",
    "        (si['os_issue_resolution_feedback'] == 'Y')\n",
    "    )\n",
    ")\n",
    "\n",
    "# si[['client_feedback_channel', 'os_issue_resolution_feedback', 'client_feedback_contradiction']].loc[si['client_feedback_contradiction'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a72eb5a-e319-4599-b916-536b62580026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service standards have volume but no volume indicated at service level\n",
    "ss_vol_by_service = (\n",
    "    ss.groupby(['fiscal_yr', 'service_id'])['total_volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'total_volume':'total_volume_ss'})\n",
    ")\n",
    "\n",
    "si = (\n",
    "    si.merge(ss_vol_by_service, on=['fiscal_yr', 'service_id'], how='left')\n",
    "    .fillna({'total_volume_ss': 0})\n",
    ")\n",
    "\n",
    "si['qa_ss_vol_without_si_vol'] = (\n",
    "    (si['total_volume_ss'] > 0) & (si['num_applications_total'] == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce19d5fd-2ee5-430f-8c71-f0aee11e65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services that target society as a recipient type we would not expect to see specific interaction volume\n",
    "# Note that this assumption may be false\n",
    "si['qa_service_recipient_type_society_with_interactions'] = (\n",
    "    (si['service_recipient_type'] == 'SOCIETY') &\n",
    "    (si['num_applications_total'] > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a494f78a-85d5-4896-ba60-e6c3e4a84b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'persons' are a client type should not be 'NA' for SIN as ID\n",
    "si['qa_use_of_sin_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('PERSON')) &\n",
    "    (si['sin_usage'].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e5cc29-4a6d-4616-a1b3-29d1c5a5131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'econom' (business) are a client type should not be 'NA' for CRA BN as ID\n",
    "si['qa_use_of_cra_bn_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('ECONOM')) &\n",
    "    (si['cra_bn_identifier_usage'].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6405f82f-3a42-49f1-9a38-1a5e7536d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataFrames to export to csv and their corresponding names\n",
    "csv_exports = {\n",
    "    \"si_qa\": si,\n",
    "    \"ss_qa\": ss,\n",
    "}\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, df in csv_exports.items():\n",
    "    # Generate the filename using the key (string name)\n",
    "    fn = f\"{name}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to CSV\n",
    "    df.to_csv(fn, index=False, sep=';')\n",
    "    \n",
    "    # Append the timestamp at the end of the file\n",
    "    with open(fn, 'a') as timestamped_file:\n",
    "        timestamped_file.write(f\"\\nTimestamp:{current_datetime_str}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
