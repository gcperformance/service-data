{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444df633-bbf1-4bd6-98b3-4873f86a42ba",
   "metadata": {},
   "source": [
    "# Quality assurance and automated service data review\n",
    "\n",
    "This notebook reviews published service data for common mistakes. Relies on gc-service-data-script outputs to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b378e41d-fa09-48a8-b639-e18450cded57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, pytz, os, re\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for JupyterLab progress bars\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3872ae-3955-4d82-a897-04bc4ccbf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datetime: 2024-12-03_13:24:50\n"
     ]
    }
   ],
   "source": [
    "# Specify date and time in correct timezone\n",
    "timezone = pytz.timezone('America/Montreal')\n",
    "current_datetime = pd.Timestamp.now(tz=timezone)\n",
    "current_datetime_str = current_datetime.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "print(f'Current datetime: {current_datetime_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bc87f5-686b-4e19-9bde-e154b2e4f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import service inventory and service standards to dataframes\n",
    "si = pd.read_csv('si.csv', sep=';',  na_values=['NA'], keep_default_na=True)\n",
    "ss = pd.read_csv('ss.csv', sep=';',  na_values=['NA'], keep_default_na=True)\n",
    "\n",
    "# Extract date of generation from timestamp on last line\n",
    "date = pd.to_datetime(si.iloc[-1, 0].split(':')[1].split('_')[0])\n",
    "\n",
    "# Remove last line with datestamp from dataframes\n",
    "si = si.iloc[:-1]\n",
    "ss = ss.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33a1ace-2a83-4b5f-ab1a-e7a271d18341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 669 new URIs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1662a4d1fd3c45b9a7400adbb55e527f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating URIs:   0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All URI checks complete. 445 were flagged as problematic.\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a URI's format is a problem\n",
    "def is_problem_format(uri):\n",
    "    # Handle blank or empty URIs\n",
    "    if not uri or uri.strip() == \"\":\n",
    "        return False  # Blank URIs are not considered problematic\n",
    "    \n",
    "    # Strip whitespace for proper checking\n",
    "    uri = uri.strip()\n",
    "    \n",
    "    # Check if input starts with valid schemes\n",
    "    if not uri.startswith(('http://', 'https://')):\n",
    "        return True  # Problematic if not starting with valid schemes\n",
    "    \n",
    "    # Check for multiple occurrences of 'http' or 'https'\n",
    "    if uri.count('http://') > 1 or uri.count('https://') > 1:\n",
    "        return True  # Problematic if more than one scheme\n",
    "    \n",
    "    # Check for invalid characters\n",
    "    invalid_characters_pattern = r'[^a-zA-Z0-9\\-._~:/?#@!$&\\'()*+,;=%]'\n",
    "    if re.search(invalid_characters_pattern, uri):\n",
    "        return True  # Problematic if invalid characters are found\n",
    "    \n",
    "    return False  # No problem\n",
    "\n",
    "\n",
    "# Function to check if a URI's address is a problem\n",
    "def is_problem_uri(uri):\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        response = session.head(uri, allow_redirects=True, timeout=60)\n",
    "        return uri, response.status_code != 200, response.status_code\n",
    "    except requests.RequestException as e:\n",
    "        return uri, True, str(e)\n",
    "\n",
    "# Load existing validation results\n",
    "try:\n",
    "    previous_results_df = pd.read_csv('uri_validation_results.csv')\n",
    "    previous_validation_results = dict(zip(previous_results_df['uri'], previous_results_df['is_problem']))\n",
    "except FileNotFoundError:\n",
    "    previous_validation_results = {}\n",
    "\n",
    "# Consolidate all URIs from DataFrames\n",
    "uri_cols = {\n",
    "    'service_uri_en': si,\n",
    "    'service_uri_fr': si,\n",
    "    'standards_targets_uri_en': ss,\n",
    "    'standards_targets_uri_fr': ss,\n",
    "    'performance_results_uri_en': ss,\n",
    "    'performance_results_uri_fr': ss\n",
    "}\n",
    "all_uris = pd.concat([df[col] for col, df in uri_cols.items()], ignore_index=True)\n",
    "unique_uris = all_uris.dropna().unique()\n",
    "problem_format_uris = [uri for uri in unique_uris if is_problem_format(uri)]\n",
    "\n",
    "# Identify new URIs that need validation\n",
    "new_uris = [uri for uri in problem_format_uris if uri not in previous_validation_results]\n",
    "print(f'Checking {len(new_uris)} new URIs.')\n",
    "\n",
    "# Validate new URIs using multithreading\n",
    "validation_results = {}\n",
    "details = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=500) as executor:\n",
    "    for uri, is_problem, detail in tqdm(\n",
    "        executor.map(is_problem_uri, new_uris),\n",
    "        total=len(new_uris),\n",
    "        desc=\"Validating URIs\"\n",
    "    ):\n",
    "        validation_results[uri] = is_problem\n",
    "        details[uri] = detail\n",
    "\n",
    "# Combine previous and new validation results\n",
    "validation_results.update(previous_validation_results)\n",
    "\n",
    "# Map validation results to DataFrames\n",
    "for column, df in uri_cols.items():\n",
    "    # Add validation status\n",
    "    df[f'qa_{column}_is_problem'] = (\n",
    "        df[column]\n",
    "        .map(validation_results)\n",
    "        .astype(bool)\n",
    "        .fillna(True)  # Default to False for unmapped URIs\n",
    "        \n",
    "    )\n",
    "    # Add validation details\n",
    "#    df[f'qa_{column}_detail'] = (\n",
    "#        df[column]\n",
    "#        .map(details)\n",
    "#        .astype(str)\n",
    "#        .fillna('No URI')  # Default message for rows without a URI\n",
    "#    )\n",
    "\n",
    "# Save updated validation results\n",
    "validation_results_df = pd.DataFrame(\n",
    "    [(uri, validation_results[uri], details.get(uri, \"No Details\")) for uri in validation_results],\n",
    "    columns=['uri', 'is_problem', 'detail']\n",
    ")\n",
    "validation_results_df.to_csv('uri_validation_results.csv', index=False)\n",
    "\n",
    "# Summary\n",
    "problematic_results = validation_results_df['is_problem'].sum()\n",
    "print(f\"All URI checks complete. {problematic_results} were flagged as problematic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98fc3e1-7124-43ee-a9e8-5628faed5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate service ID conflict\n",
    "# Step 1: Flag rows where 'service_id' is duplicated within each 'fiscal_yr'\n",
    "si['qa_duplicate_sid'] = si.duplicated(subset=['fiscal_yr', 'service_id'], keep=False)\n",
    "\n",
    "# Step 2: Get unique 'service_id's that are flagged as duplicates\n",
    "duplicate_ids = si.loc[si['qa_duplicate_sid'], 'service_id'].unique()\n",
    "\n",
    "# Step 3: Filter rows with duplicate 'service_id's and group by 'service_id' and 'department_en'\n",
    "duplicate_groups = (\n",
    "    si.loc[si['service_id'].isin(duplicate_ids), ['fiscal_yr', 'service_id', 'department_en']]\n",
    "    .groupby(['service_id', 'department_en'])['fiscal_yr']  # Count occurrences of 'fiscal_yr'\n",
    "    .nunique()  # Count unique fiscal years for each group\n",
    ")\n",
    "\n",
    "# Step 4: Identify groups with only one unique fiscal year (problematic cases)\n",
    "problematic_duplicates = duplicate_groups[duplicate_groups == 1].reset_index()\n",
    "\n",
    "# Step 5: Keep only 'service_id' and 'department_en' columns\n",
    "problematic_duplicates = problematic_duplicates[['service_id', 'department_en']]\n",
    "\n",
    "# Step 6: Create a set of tuples from 'problematic_duplicates' for efficient lookup\n",
    "problematic_set = set(zip(problematic_duplicates['service_id'], problematic_duplicates['department_en']))\n",
    "\n",
    "# Step 7: Update the 'qa_duplicate_sid' column based on whether each row matches a problematic duplicate\n",
    "si['qa_duplicate_sid'] = si.apply(\n",
    "    lambda row: (row['service_id'], row['department_en']) in problematic_set, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# check:\n",
    "# si.loc[:, ['fiscal_yr', 'department_en', 'service_id', 'qa_duplicate_sid']][si['qa_duplicate_sid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fb67d2-2868-4857-9926-3f361e2aec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Service Standard ID conflict\n",
    "# Step 1: Flag rows where 'service_standard_id' is duplicated within each 'fiscal_yr'\n",
    "ss['qa_duplicate_stdid'] = ss.duplicated(subset=['fiscal_yr', 'service_standard_id'], keep=False)\n",
    "\n",
    "# Step 2: Get unique 'service_standard_id's that are flagged as duplicates\n",
    "duplicate_ids = ss.loc[ss['qa_duplicate_stdid'], 'service_standard_id'].unique()\n",
    "\n",
    "# Step 3: Filter rows with duplicate 'service_standard_id's and group by 'service_standard_id' and 'department_en'\n",
    "duplicate_groups = (\n",
    "    ss.loc[ss['service_standard_id'].isin(duplicate_ids), ['fiscal_yr', 'service_standard_id', 'department_en']]\n",
    "    .groupby(['service_standard_id', 'department_en'])['fiscal_yr']  # Count occurrences of 'fiscal_yr'\n",
    "    .nunique()  # Count unique fiscal years for each group\n",
    ")\n",
    "\n",
    "# Step 4: Identify groups with only one unique fiscal year (problematic cases)\n",
    "problematic_duplicates = duplicate_groups[duplicate_groups == 1].reset_index()\n",
    "\n",
    "# Step 5: Keep only 'service_standard_id' and 'department_en' columns\n",
    "problematic_duplicates = problematic_duplicates[['service_standard_id', 'department_en']]\n",
    "\n",
    "# Step 6: Create a set of tuples from 'problematic_duplicates' for efficient lookup\n",
    "problematic_set = set(zip(problematic_duplicates['service_standard_id'], problematic_duplicates['department_en']))\n",
    "\n",
    "# Step 7: Update the 'qa_duplicate_sid' column based on whether each row matches a problematic duplicate\n",
    "ss['qa_duplicate_stdid'] = ss.apply(\n",
    "    lambda row: (row['service_standard_id'], row['department_en']) in problematic_set, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# check:\n",
    "# ss.loc[:, ['fiscal_yr', 'department_en', 'service_id', 'service_standard_id', 'qa_duplicate_stdid']][ss['qa_duplicate_stdid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952cff2c-5b01-4b80-b4c3-47c4c266f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record is reported for a fiscal year that is incomplete or in the future.\n",
    "si['fiscal_yr_end_date'] = pd.to_datetime(si['fiscal_yr'].str.split('-').str[1]+'-04-01')\n",
    "si['qa_fiscal_yr_in_future'] = si['fiscal_yr_end_date'] >= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9ad843-c9aa-40ea-a1fd-2d8d8885f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record has contradiction between client feedback channels and online interaction points for feedback\n",
    "si['qa_client_feedback_contradiction'] = (\n",
    "\n",
    "    # Service accepts client feedback via the online channel (ONL) but online issue resolution or feedback is not applicable or not activated\n",
    "    (\n",
    "        si['client_feedback_channel'].str.contains('ONL') & \n",
    "        (\n",
    "            si['os_issue_resolution_feedback'].isna() | \n",
    "            (si['os_issue_resolution_feedback'] == 'N')\n",
    "        )\n",
    "    ) |\n",
    "    # Service has not listed the online channel (ONL) for client feedback but online issue resolution or feedback is activated\n",
    "    (\n",
    "        (~si['client_feedback_channel'].str.contains('ONL')) &\n",
    "        (si['os_issue_resolution_feedback'] == 'Y')\n",
    "    )\n",
    ")\n",
    "\n",
    "# si[['client_feedback_channel', 'os_issue_resolution_feedback', 'client_feedback_contradiction']].loc[si['client_feedback_contradiction'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a72eb5a-e319-4599-b916-536b62580026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service standards have volume but no volume indicated at service level\n",
    "ss_vol_by_service = (\n",
    "    ss.groupby(['fiscal_yr', 'service_id'])['total_volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'total_volume':'total_volume_ss'})\n",
    ")\n",
    "\n",
    "si = (\n",
    "    si.merge(ss_vol_by_service, on=['fiscal_yr', 'service_id'], how='left')\n",
    "    .fillna({'total_volume_ss': 0})\n",
    ")\n",
    "\n",
    "si['qa_ss_vol_without_si_vol'] = (\n",
    "    (si['total_volume_ss'] > 0) & (si['num_applications_total'] == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce19d5fd-2ee5-430f-8c71-f0aee11e65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services that target society as a recipient type we would not expect to see specific interaction volume\n",
    "# Note that this assumption may be false\n",
    "si['qa_service_recipient_type_society_with_interactions'] = (\n",
    "    (si['service_recipient_type'] == 'SOCIETY') &\n",
    "    (si['num_applications_total'] > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a494f78a-85d5-4896-ba60-e6c3e4a84b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'persons' are a client type should not be 'NA' for SIN as ID\n",
    "si['qa_use_of_sin_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('PERSON')) &\n",
    "    (si['sin_usage'].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e5cc29-4a6d-4616-a1b3-29d1c5a5131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'econom' (business) are a client type should not be 'NA' for CRA BN as ID\n",
    "si['qa_use_of_cra_bn_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('ECONOM')) &\n",
    "    (si['cra_bn_identifier_usage'].isna())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6405f82f-3a42-49f1-9a38-1a5e7536d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataFrames to export to csv and their corresponding names\n",
    "csv_exports = {\n",
    "    \"si_qa\": si,\n",
    "    \"ss_qa\": ss,\n",
    "}\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, df in csv_exports.items():\n",
    "    # Generate the filename using the key (string name)\n",
    "    fn = f\"{name}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to CSV\n",
    "    df.to_csv(fn, index=False, sep=';')\n",
    "    \n",
    "    # Append the timestamp at the end of the file\n",
    "    with open(fn, 'a') as timestamped_file:\n",
    "        timestamped_file.write(f\"\\nTimestamp:{current_datetime_str}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
