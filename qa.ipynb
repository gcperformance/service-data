{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444df633-bbf1-4bd6-98b3-4873f86a42ba",
   "metadata": {},
   "source": [
    "# Quality assurance and automated service data review\n",
    "\n",
    "This notebook reviews published service data for common mistakes. Relies on gc-service-data-script outputs to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b378e41d-fa09-48a8-b639-e18450cded57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, pytz, os, re\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for JupyterLab progress bars\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3872ae-3955-4d82-a897-04bc4ccbf131",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datetime: 2025-01-17_14:36:50\n"
     ]
    }
   ],
   "source": [
    "# Specify date and time in correct timezone\n",
    "timezone = pytz.timezone('America/Montreal')\n",
    "current_datetime = pd.Timestamp.now(tz=timezone)\n",
    "current_datetime_str = current_datetime.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "print(f'Current datetime: {current_datetime_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bc87f5-686b-4e19-9bde-e154b2e4f428",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import service inventory and service standards to dataframes\n",
    "si = pd.read_csv('si.csv', sep=';',  na_values=[], keep_default_na=False)\n",
    "ss = pd.read_csv('ss.csv', sep=';',  na_values=[], keep_default_na=False)\n",
    "qa_issues_description = pd.read_csv('qa_issues_descriptions.csv')\n",
    "\n",
    "# Extract date of generation from timestamp on last line\n",
    "date = pd.to_datetime(si.iloc[-1, 0].split(':')[1].split('_')[0])\n",
    "\n",
    "# Remove last line with datestamp from dataframes\n",
    "si = si.iloc[:-1]\n",
    "ss = ss.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1ab56a-7ad8-4a45-aa9c-9a696576d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce all numeric fields\n",
    "int_cols = {\n",
    "    'num_phone_enquiries': si,\n",
    "    'num_applications_by_phone': si,\n",
    "    'num_website_visits': si,\n",
    "    'num_applications_online': si,\n",
    "    'num_applications_by_mail': si,\n",
    "    'num_applications_by_email': si,\n",
    "    'num_applications_by_fax': si,\n",
    "    'num_applications_by_other': si,\n",
    "    'num_applications_total': si,\n",
    "    'volume_meeting_target': ss,\n",
    "    'total_volume': ss\n",
    "}\n",
    "\n",
    "# int_cols[column][column]\n",
    "# is int_cols[column] = dict[key],\n",
    "# then dict[key] = dataframe, \n",
    "# so dict[key][column] = dataframe[column]\n",
    "\n",
    "for column, df in int_cols.items():\n",
    "    int_cols[column][column] = pd.to_numeric(df[column], errors = 'coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8b5c63-cdbf-45a2-92bc-a1628bd5d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string ids to numeric, strip out prefixes\n",
    "si['service_id_numeric'] = si['service_id'].str.replace(r'^SRV', '', regex=True)\n",
    "si['service_id_numeric'] = pd.to_numeric(si['service_id_numeric'], errors = 'coerce')\n",
    "\n",
    "ss['service_standard_id_numeric'] = ss['service_standard_id'].str.replace(r'^STAN', '', regex=True)\n",
    "ss['service_standard_id_numeric'] = pd.to_numeric(ss['service_standard_id_numeric'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98fc3e1-7124-43ee-a9e8-5628faed5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate service ID conflict\n",
    "# Step 1: Flag rows where 'service_id' is duplicated within each 'fiscal_yr'\n",
    "si['qa_duplicate_sid'] = si.duplicated(subset=['fiscal_yr', 'service_id'], keep=False)\n",
    "\n",
    "# Step 2: Get unique 'service_id's that are flagged as duplicates\n",
    "duplicate_ids = si.loc[si['qa_duplicate_sid'], 'service_id'].unique()\n",
    "\n",
    "# Step 3: Filter rows with duplicate 'service_id's and group by 'service_id' and 'department_en'\n",
    "duplicate_groups = (\n",
    "    si.loc[si['service_id'].isin(duplicate_ids), ['fiscal_yr', 'service_id', 'department_en']]\n",
    "    .groupby(['service_id', 'department_en'])['fiscal_yr']  # Count occurrences of 'fiscal_yr'\n",
    "    .nunique()  # Count unique fiscal years for each group\n",
    ")\n",
    "\n",
    "# Step 4: Identify groups with only one unique fiscal year (problematic cases)\n",
    "problematic_duplicates = duplicate_groups[duplicate_groups == 1].reset_index()\n",
    "\n",
    "# Step 5: Keep only 'service_id' and 'department_en' columns\n",
    "problematic_duplicates = problematic_duplicates[['service_id', 'department_en']]\n",
    "\n",
    "# Step 6: Create a set of tuples from 'problematic_duplicates' for efficient lookup\n",
    "problematic_set = set(zip(problematic_duplicates['service_id'], problematic_duplicates['department_en']))\n",
    "\n",
    "# Step 7: Update the 'qa_duplicate_sid' column based on whether each row matches a problematic duplicate\n",
    "si['qa_duplicate_sid'] = si.apply(\n",
    "    lambda row: (row['service_id'], row['department_en']) in problematic_set, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# check:\n",
    "# si.loc[:, ['fiscal_yr', 'department_en', 'service_id', 'qa_duplicate_sid']][si['qa_duplicate_sid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fb67d2-2868-4857-9926-3f361e2aec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Service Standard ID conflict\n",
    "# Step 1: Flag rows where 'service_standard_id' is duplicated within each 'fiscal_yr'\n",
    "ss['qa_duplicate_stdid'] = ss.duplicated(subset=['fiscal_yr', 'service_standard_id_numeric'], keep=False)\n",
    "\n",
    "# Step 2: Get unique 'service_standard_id's that are flagged as duplicates\n",
    "duplicate_ids = ss.loc[ss['qa_duplicate_stdid'], 'service_standard_id_numeric'].unique()\n",
    "\n",
    "# Step 3: Filter rows with duplicate 'service_standard_id's and group by 'service_standard_id' and 'department_en'\n",
    "duplicate_groups = (\n",
    "    ss.loc[ss['service_standard_id_numeric'].isin(duplicate_ids), ['fiscal_yr', 'service_standard_id_numeric', 'department_en']]\n",
    "    .groupby(['service_standard_id_numeric', 'department_en'])['fiscal_yr']  # Count occurrences of 'fiscal_yr'\n",
    "    .nunique()  # Count unique fiscal years for each group\n",
    ")\n",
    "\n",
    "# Step 4: Identify groups with only one unique fiscal year (problematic cases)\n",
    "problematic_duplicates = duplicate_groups[duplicate_groups == 1].reset_index()\n",
    "\n",
    "# Step 5: Keep only 'service_standard_id' and 'department_en' columns\n",
    "problematic_duplicates = problematic_duplicates[['service_standard_id_numeric', 'department_en']]\n",
    "\n",
    "# Step 6: Create a set of tuples from 'problematic_duplicates' for efficient lookup\n",
    "problematic_set = set(zip(problematic_duplicates['service_standard_id_numeric'], problematic_duplicates['department_en']))\n",
    "\n",
    "# Step 7: Update the 'qa_duplicate_sid' column based on whether each row matches a problematic duplicate\n",
    "ss['qa_duplicate_stdid'] = ss.apply(\n",
    "    lambda row: (row['service_standard_id_numeric'], row['department_en']) in problematic_set, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# check:\n",
    "# ss.loc[:, ['fiscal_yr', 'department_en', 'service_id', 'service_standard_id', 'qa_duplicate_stdid']][ss['qa_duplicate_stdid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5314801-186c-4b39-b7ba-a21b4ea371ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify service IDs that have already been used by other departments in previous fiscal years\n",
    "si_filtered = si[['service_id', 'department_en', 'org_id', 'fiscal_yr']]\n",
    "si_filtered = si_filtered.sort_values(by=['service_id', 'fiscal_yr']).reset_index(drop=True)\n",
    "\n",
    "# Self-join to compare records\n",
    "joined_df = si_filtered.merge(si_filtered, on='service_id', suffixes=('', '_prev'))\n",
    "\n",
    "# Filter reused records\n",
    "reused_ids = joined_df[\n",
    "    (joined_df['fiscal_yr'] > joined_df['fiscal_yr_prev']) & \n",
    "    (joined_df['department_en'] != joined_df['department_en_prev'])\n",
    "]\n",
    "\n",
    "# Select the record with the latest 'fiscal_yr_prev' for each 'service_id' and 'fiscal_yr'\n",
    "reused_ids = reused_ids.loc[reused_ids.groupby(['service_id', 'fiscal_yr'])['fiscal_yr_prev'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# Identify which fiscal year and department previously used the id\n",
    "reused_ids['reused_id_from'] = reused_ids['fiscal_yr_prev']+' '+reused_ids['department_en_prev']\n",
    "\n",
    "# Create a unique key for matching\n",
    "reused_ids['key'] = (\n",
    "    reused_ids['fiscal_yr'].astype(str)+' '+\n",
    "    reused_ids['org_id'].astype(str)+' '+\n",
    "    reused_ids['service_id'].astype(str)\n",
    "    )\n",
    "\n",
    "si['key'] = (\n",
    "    si['fiscal_yr'].astype(str)+' '+\n",
    "    si['org_id'].astype(str)+' '+\n",
    "    si['service_id'].astype(str)\n",
    "    )\n",
    "\n",
    "# Map 'reused_id_from' to the original 'si' DataFrame\n",
    "reused_id_from_dict = dict(zip(reused_ids['key'], reused_ids['reused_id_from']))\n",
    "\n",
    "si['reused_id_from'] = si['key'].map(reused_id_from_dict)\n",
    "si['qa_reused_sid'] = si['reused_id_from'].notna()\n",
    "\n",
    "# Drop the temporary key column\n",
    "si = si.drop(columns=['key'])\n",
    "\n",
    "# check\n",
    "# si[['fiscal_yr', 'service_id', 'department_en', 'qa_reused_sid']][si['qa_reused_sid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b964c6-63af-4561-a2c3-868c0198cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify service standard IDs that have already been used by other departments in previous fiscal years\n",
    "# ss_filtered = ss[['service_standard_id', 'department_en', 'org_id', 'fiscal_yr']]\n",
    "# ss_filtered = ss_filtered.sort_values(by=['service_standard_id', 'fiscal_yr']).reset_index(drop=True)\n",
    "\n",
    "# # Self-join to compare records\n",
    "# joined_df = ss_filtered.merge(ss_filtered, on='service_standard_id', suffixes=('', '_prev'))\n",
    "\n",
    "# # Filter reused records\n",
    "# reused_ids = joined_df[\n",
    "#     (joined_df['fiscal_yr'] > joined_df['fiscal_yr_prev']) & \n",
    "#     (joined_df['department_en'] != joined_df['department_en_prev'])\n",
    "# ]\n",
    "\n",
    "# # Select the record with the latest 'fiscal_yr_prev' for each 'service_standard_id' and 'fiscal_yr'\n",
    "# reused_ids = reused_ids.loc[reused_ids.groupby(['service_standard_id', 'fiscal_yr'])['fiscal_yr_prev'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# # Identify which fiscal year and department previously used the id\n",
    "# reused_ids['reused_id_from'] = reused_ids['fiscal_yr_prev']+' '+reused_ids['department_en_prev']\n",
    "\n",
    "# # Create a unique key for matching\n",
    "# reused_ids['key'] = reused_ids['fiscal_yr']+' '+reused_ids['org_id']+' '+reused_ids['service_standard_id']\n",
    "# ss['key'] = ss['fiscal_yr']+' '+ss['org_id']+' '+ss['service_standard_id']\n",
    "\n",
    "# # Map 'reused_id_from' to the original 'si' DataFrame\n",
    "# reused_id_from_dict = dict(zip(reused_ids['key'], reused_ids['reused_id_from']))\n",
    "\n",
    "# ss['reused_id_from'] = ss['key'].map(reused_id_from_dict)\n",
    "# ss['qa_reused_stdid'] = ss['reused_id_from'].notna()\n",
    "\n",
    "# # Drop the temporary key column\n",
    "# ss = ss.drop(columns=['key'])\n",
    "\n",
    "# #check\n",
    "# #ss[['fiscal_yr', 'service_id', 'service_standard_id', 'department_en', 'qa_reused_stdid']][ss['qa_reused_stdid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952cff2c-5b01-4b80-b4c3-47c4c266f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record is reported for a fiscal year that is incomplete or in the future.\n",
    "si['fiscal_yr_end_date'] = pd.to_datetime(si['fiscal_yr'].str.split('-').str[1]+'-04-01')\n",
    "si['qa_si_fiscal_yr_in_future'] = si['fiscal_yr_end_date'] >= date\n",
    "\n",
    "ss['fiscal_yr_end_date'] = pd.to_datetime(ss['fiscal_yr'].str.split('-').str[1]+'-04-01')\n",
    "ss['qa_ss_fiscal_yr_in_future'] = ss['fiscal_yr_end_date'] >= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9ad843-c9aa-40ea-a1fd-2d8d8885f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record has contradiction between client feedback channels and online interaction points for feedback\n",
    "si['qa_client_feedback_contradiction'] = (\n",
    "\n",
    "    # Service accepts client feedback via the online channel (ONL) but online issue resolution or feedback is not applicable or not activated\n",
    "    (\n",
    "        si['client_feedback_channel'].str.contains('ONL') & \n",
    "        (\n",
    "            si['os_issue_resolution_feedback'].isna() | \n",
    "            (si['os_issue_resolution_feedback'] == 'N')\n",
    "        )\n",
    "    ) |\n",
    "    # Service has not listed the online channel (ONL) for client feedback but online issue resolution or feedback is activated\n",
    "    (\n",
    "        (~si['client_feedback_channel'].str.contains('ONL')) &\n",
    "        (si['os_issue_resolution_feedback'] == 'Y')\n",
    "    )\n",
    ")\n",
    "\n",
    "# si[['client_feedback_channel', 'os_issue_resolution_feedback', 'client_feedback_contradiction']].loc[si['client_feedback_contradiction'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a72eb5a-e319-4599-b916-536b62580026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service standards have volume but no volume indicated at service level\n",
    "ss_vol_by_service = (\n",
    "    ss.groupby(['fiscal_yr', 'service_id'])['total_volume']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'total_volume':'total_volume_ss'})\n",
    ")\n",
    "\n",
    "si = si.merge(ss_vol_by_service, on=['fiscal_yr', 'service_id'], how='left').fillna(0)\n",
    "\n",
    "si['qa_ss_vol_without_si_vol'] = (\n",
    "    (si['total_volume_ss'] > 0) & (si['num_applications_total'] == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aff6424-e048-44a8-beea-4881a167e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service standard reports no volume\n",
    "ss['qa_no_ss_volume'] = (ss['total_volume'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce19d5fd-2ee5-430f-8c71-f0aee11e65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services that target society as a recipient type we would not expect to see specific interaction volume\n",
    "# Note that this assumption may be false\n",
    "si['num_applications_total'] = pd.to_numeric(si['num_applications_total'], errors = 'coerce').fillna(0).astype(int)\n",
    "\n",
    "si['qa_service_recipient_type_society_with_interactions'] = (\n",
    "    (si['service_recipient_type'] == 'SOCIETY') &\n",
    "    (si['num_applications_total'] > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a494f78a-85d5-4896-ba60-e6c3e4a84b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'persons' are a client type should not be 'NA' for SIN as ID\n",
    "si['qa_use_of_sin_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('PERSON')) &\n",
    "    (si['sin_usage'] == 'NA')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e5cc29-4a6d-4616-a1b3-29d1c5a5131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services where 'econom' (business) are a client type should not be 'NA' for CRA BN as ID\n",
    "si['qa_use_of_cra_bn_applicable'] = (\n",
    "    (si['client_target_groups'].str.contains('ECONOM')) &\n",
    "    (si['cra_bn_identifier_usage'] == 'NA')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8327a5-e413-487d-9c27-9270d69e9368",
   "metadata": {},
   "source": [
    "### Clean QA report\n",
    "\n",
    "In order to have a clean report of issues to send to departments & agencies, the following bit of script re-organizes the information in the qa columns to a simple report for 2023-2024 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a732e1a-7921-4963-bf5d-951256c89127",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_qa_cols = si.columns.str.startswith('qa')\n",
    "ss_qa_cols = ss.columns.str.startswith('qa')\n",
    "\n",
    "critical_si_qa_cols = [\n",
    "    'qa_duplicate_sid',\n",
    "    'qa_si_fiscal_yr_in_future',\n",
    "    'qa_ss_vol_without_si_vol',\n",
    "    'qa_reused_sid'\n",
    "]\n",
    "\n",
    "critical_ss_qa_cols = [\n",
    "    'qa_duplicate_stdid',\n",
    "    'qa_no_ss_volume',\n",
    "    'qa_ss_fiscal_yr_in_future'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b2a2a48-5e2b-4a17-a9c1-9738969f51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing SI QA report\n",
    "si_report_cols = [\n",
    "    'department_en',\n",
    "    'fiscal_yr', \n",
    "    'service_id', \n",
    "    'service_name_en', \n",
    "    'service_name_fr',\n",
    "    'num_applications_total',\n",
    "    'total_volume_ss',\n",
    "    'reused_id_from'\n",
    "]\n",
    "\n",
    "si_qa_report = pd.melt(si, id_vars=si_report_cols, value_vars=critical_si_qa_cols, var_name='issue', value_name='issue_present')\n",
    "\n",
    "si_qa_report = si_qa_report[(si_qa_report['issue_present'] & si_qa_report['fiscal_yr'].isin(['2023-2024', '2024-2025']))]\n",
    "\n",
    "si_qa_report = pd.merge(\n",
    "    si_qa_report, \n",
    "    qa_issues_description.loc[:, [\n",
    "        'qa_field_name', \n",
    "        'description_en', \n",
    "        'action_en',\n",
    "        'description_fr',\n",
    "        'action_fr'\n",
    "    ]], \n",
    "    left_on='issue', \n",
    "    right_on='qa_field_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "si_qa_report = si_qa_report.drop(columns=['issue_present', 'qa_field_name'])\n",
    "\n",
    "si_qa_report = si_qa_report.sort_values(by=['department_en', 'service_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86be24c1-5e63-443b-94de-4cd67677ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing SS QA report\n",
    "ss_report_cols = [\n",
    "    'department_en',\n",
    "    'fiscal_yr', \n",
    "    'service_id', \n",
    "    'service_name_en', \n",
    "    'service_name_fr',\n",
    "    'service_standard_id',\n",
    "    'service_standard_en',\n",
    "    'service_standard_fr',\n",
    "    'volume_meeting_target',\n",
    "    'total_volume',\n",
    "]\n",
    "\n",
    "ss_qa_report = pd.melt(ss, id_vars=ss_report_cols, value_vars=critical_ss_qa_cols, var_name='issue', value_name='issue_present')\n",
    "\n",
    "ss_qa_report = ss_qa_report[(ss_qa_report['issue_present'] & ss_qa_report['fiscal_yr'].isin(['2023-2024', '2024-2025']))]\n",
    "\n",
    "ss_qa_report = pd.merge(\n",
    "    ss_qa_report, \n",
    "    qa_issues_description.loc[:, [\n",
    "        'qa_field_name', \n",
    "        'description_en', \n",
    "        'action_en',\n",
    "        'description_fr',\n",
    "        'action_fr'\n",
    "    ]], \n",
    "    left_on='issue', \n",
    "    right_on='qa_field_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "ss_qa_report = ss_qa_report.drop(columns=['issue_present', 'qa_field_name'])\n",
    "\n",
    "ss_qa_report = ss_qa_report.sort_values(by=['department_en', 'service_id', 'service_standard_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228610c9-ad2c-4efa-b03e-fe9b96bfa423",
   "metadata": {},
   "source": [
    "## Export data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6405f82f-3a42-49f1-9a38-1a5e7536d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataFrames to export to csv and their corresponding names\n",
    "csv_exports = {\n",
    "    \"si_qa\": si,\n",
    "    \"ss_qa\": ss,\n",
    "    \"si_qa_report\": si_qa_report,\n",
    "    \"ss_qa_report\": ss_qa_report\n",
    "}\n",
    "\n",
    "# Loop through the dictionary\n",
    "for name, df in csv_exports.items():\n",
    "    # Generate the filename using the key (string name)\n",
    "    fn = f\"{name}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to CSV\n",
    "    df.to_csv(fn, index=False, sep=';')\n",
    "    \n",
    "    # Append the timestamp at the end of the file\n",
    "    with open(fn, 'a') as timestamped_file:\n",
    "        timestamped_file.write(f\"\\nTimestamp:{current_datetime_str}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
